{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center> <img src=\"logo.png\" width=\"200\"></center> \n",
    "\n",
    "<center> <h1>Logistic Regression with Gradient Ascent</h1> </center>\n",
    "<center> <h1>Handwritten Digits Classification</h1> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient Ascent algorithm\n",
    "From our [video](https://www.youtube.com/watch?v=TM1lijyQnaI&t=810s) on logistic regression, we derived the equation to update the logistic regression model parameters as:    \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\alpha (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This maximizes the following log likelihood function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\sum_{i=1}^{m}y_i\\log(h(x_{i})) + (1 - y_i)\\log(1 - h(x_{i}))\n",
    "\\end{equation}\n",
    "\n",
    "where our hypothesis is a sigmoid function\n",
    "\\begin{equation}\n",
    "h(x_i) = \\frac{1}{1 + e^{\\theta^T \\bar{x}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batch gradient Ascent\n",
    "```FOR j FROM 0 -> max_iteration: \n",
    "    FOR i FROM 0 -> m: \n",
    "        theta += (alpha) * (y[i] - h(x[i])) * x_bar\n",
    "    ENDLOOP\n",
    "ENDLOOP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multi-class Classificaton with one-vs-all (one-vs-rest)\n",
    "If you have n-classes, we train n-classifiers and given a new data point we predict using all the classifiers and choose the one with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "f, axarr = plt.subplots(3,3)\n",
    "axarr[0,0].imshow(digits.images[0])\n",
    "axarr[0,1].imshow(digits.images[1])\n",
    "axarr[0,2].imshow(digits.images[2])\n",
    "axarr[1,0].imshow(digits.images[3])\n",
    "axarr[1,1].imshow(digits.images[4])\n",
    "axarr[1,2].imshow(digits.images[5])\n",
    "axarr[2,0].imshow(digits.images[6])\n",
    "axarr[2,1].imshow(digits.images[7])\n",
    "axarr[2,2].imshow(digits.images[8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"Class for training and using a model for logistic regression\"\"\"\n",
    "    \n",
    "    def set_values(self, initial_params, alpha=0.01, max_iter=5000, class_of_interest=0):\n",
    "        \"\"\"Set the values for initial params, step size, maximum iteration, and class of interest\"\"\"\n",
    "        self.params = initial_params\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.class_of_interest = class_of_interest\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Sigmoide function\"\"\"\n",
    "        \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, x_bar, params):\n",
    "        \"\"\"predict the probability of a class\"\"\"  \n",
    "                \n",
    "        return self._sigmoid(np.dot(params, x_bar))\n",
    "    \n",
    "    def _compute_cost(self, input_var, output_var, params):\n",
    "        \"\"\"Compute the log likelihood cost\"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for x, y in zip(input_var, output_var):\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, params)\n",
    "            \n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat)\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def train(self, input_var, label, print_iter = 5000):\n",
    "        \"\"\"Train the model using batch gradient ascent\"\"\"\n",
    "        \n",
    "        iteration = 1\n",
    "        while iteration < self.max_iter:\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f'iteration: {iteration}')\n",
    "                print(f'cost: {self._compute_cost(input_var, label, self.params)}')\n",
    "                print('--------------------------------------------')\n",
    "            \n",
    "            for i, xy in enumerate(zip(input_var, label)):\n",
    "                x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "                y_hat = self.predict(x_bar, self.params)\n",
    "                \n",
    "                y_binary = 1.0 if xy[1] == self.class_of_interest else 0.0\n",
    "                gradient = (y_binary - y_hat) * x_bar\n",
    "                self.params += self.alpha * gradient\n",
    "            \n",
    "            iteration +=1\n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    def test(self, input_test, label_test):\n",
    "        \"\"\"Test the accuracy of the model using test data\"\"\"\n",
    "        self.total_classifications = 0\n",
    "        self.correct_classifications = 0\n",
    "        \n",
    "        for x,y in zip(input_test, label_test):\n",
    "            self.total_classifications += 1\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, self.params)\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            \n",
    "            if y_hat >= 0.5 and  y_binary == 1:\n",
    "                # correct classification of class_of_interest\n",
    "                self.correct_classifications += 1\n",
    "              \n",
    "            if y_hat < 0.5 and  y_binary != 1:\n",
    "                # correct classification of an other class\n",
    "                self.correct_classifications += 1\n",
    "                \n",
    "        self.accuracy = self.correct_classifications / self.total_classifications\n",
    "            \n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# split the data to training and test sets\n",
    "digits_train, digits_test, digits_label_train, digits_label_test =\\\n",
    "train_test_split(digits.data, digits.target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train a classifier for the ZERO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_0 = LogisticRegression()\n",
    "digits_regression_model_0.set_values(params_0, alpha, max_iter, 0)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_0.train(digits_train / 16.0, digits_label_train, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "digits_accuracy = digits_regression_model_0.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a ZERO digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train a classifier for the ONE digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_1 = LogisticRegression()\n",
    "digits_regression_model_1.set_values(params_0, alpha, max_iter, 1)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_1.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#accuracy\n",
    "digits_accuracy = digits_regression_model_1.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a ONE digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train a classifier for the TWO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_2 = LogisticRegression()\n",
    "digits_regression_model_2.set_values(params_0, alpha, max_iter, 2)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_2.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "digits_accuracy = digits_regression_model_2.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a TWO digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train a classifier for the EIGHT digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_8 = LogisticRegression()\n",
    "digits_regression_model_8.set_values(params_0, alpha, max_iter, 8)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_8.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "digits_accuracy = digits_regression_model_8.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a EIGHT digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
